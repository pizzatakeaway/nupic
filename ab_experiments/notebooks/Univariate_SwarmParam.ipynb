{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swarming test for HTM Univariate\n",
    "\n",
    "In this notebook we are testing:\n",
    "1. Swarm over datasets: medium, all data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# from tqdm import tqdm_notebook\n",
    "# import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Load Data and Groundtruth labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_relative = 'realKnownCause/machine_temperature_system_failure.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/NAB/' + PATH_relative) #parse_dates=True\n",
    "with open('../labels/NAB/combined_windows.json') as f:\n",
    "    labels = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>datetime</td>\n",
       "      <td>float</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-12-02 21:15:00</td>\n",
       "      <td>73.96732207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-12-02 21:20:00</td>\n",
       "      <td>74.93588199999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-12-02 21:25:00</td>\n",
       "      <td>76.12416182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp              value\n",
       "0             datetime              float\n",
       "1                    T                NaN\n",
       "2  2013-12-02 21:15:00        73.96732207\n",
       "3  2013-12-02 21:20:00  74.93588199999998\n",
       "4  2013-12-02 21:25:00        76.12416182"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([0,1], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['value'] = pd.to_numeric(df['value'])\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groundtruth labels for anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'2013-12-10 06:25:00.000000', u'2013-12-12 05:35:00.000000'],\n",
       " [u'2013-12-15 17:50:00.000000', u'2013-12-17 17:00:00.000000'],\n",
       " [u'2014-01-27 14:20:00.000000', u'2014-01-29 13:30:00.000000'],\n",
       " [u'2014-02-07 14:55:00.000000', u'2014-02-09 14:05:00.000000']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[PATH_relative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = np.zeros_like(df.value)\n",
    "\n",
    "# set values within the range = 1\n",
    "for i in range(len(labels[PATH_relative])):\n",
    "    df.loc[(df['timestamp'] >= labels[PATH_relative][i][0]) & \n",
    "             (df['timestamp'] <= labels[PATH_relative][i][1]), 'labels'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-12-02 21:15:00</td>\n",
       "      <td>73.967322</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-12-02 21:20:00</td>\n",
       "      <td>74.935882</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-12-02 21:25:00</td>\n",
       "      <td>76.124162</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-12-02 21:30:00</td>\n",
       "      <td>78.140707</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-12-02 21:35:00</td>\n",
       "      <td>79.329836</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp      value  labels\n",
       "0 2013-12-02 21:15:00  73.967322     0.0\n",
       "1 2013-12-02 21:20:00  74.935882     0.0\n",
       "2 2013-12-02 21:25:00  76.124162     0.0\n",
       "3 2013-12-02 21:30:00  78.140707     0.0\n",
       "4 2013-12-02 21:35:00  79.329836     0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SWARM\n",
    "**Hyperparameter search**\n",
    "\n",
    "Making use of the *swarm algorithm*, we will find the hyperparmeter gor our HTM model.  \n",
    "To perform the *swarm*, we need to create a `.json` file feed with informations about our data.  \n",
    "Once the hyperparameter search is completed, we need to load the best model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'realKnownCause/machine_temperature_system_failure.csv'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_def(data, selectCols, source, predictCol=[], \n",
    "                      timestampCol=None, inferenceType='TemporalAnomaly', \n",
    "                      iterationCount=-1, swarmSize='medium'):\n",
    "    \"\"\"\n",
    "    Generates the `search_def` dict, to be then converted as `.json`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas.DataFrame\n",
    "        Timestamp column if present must be named 'timestamp'.\n",
    "        Other columns are considered as `float`.\n",
    "    timestampCol: str \n",
    "        If present, name of column containing `datetime` variable.\n",
    "    selectCols: list of str\n",
    "        List of column names elegible for swarming.\n",
    "    predictCol: str\n",
    "        Name of the column --only 1!-- to be predicted.\n",
    "    source: str\n",
    "        `Path_to_file + file.csv` containing our DataFrame.\n",
    "        Must start from the root of nupic.\n",
    "    inferenceType: str, default='TemporalMultistep'\n",
    "    iterationCount: int, default=-1\n",
    "        Number of value every model iterates on. \n",
    "        `-1` iterates over all data in data frame.\n",
    "    swarmSize: str ('small', 'medium', 'large')\n",
    "        Default='medium', for debugging suggested 'small'.\n",
    "                \n",
    "    Output\n",
    "    ------\n",
    "    search_def : dict\n",
    "        Dictionary version of 'search_def.json'\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    To understand how to change the default paramaters for `inferenceType`, \n",
    "    'iterationCount' and 'swarmSize' consider reading the swarm documentation: \n",
    "    'http://nupic.docs.numenta.org/stable/guides/swarming/running.html#the-swarm-description' \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    includedFields = []\n",
    "    \n",
    "    if timestampCol != None:\n",
    "        field = {\n",
    "            \"fieldName\": timestampCol,\n",
    "            \"fieldType\": \"datetime\",\n",
    "        }\n",
    "        includedFields.append(field)\n",
    "\n",
    "    for i in selectCols:\n",
    "        field = {\n",
    "            \"fieldName\": data[i].name,\n",
    "            \"fieldType\": \"float\",\n",
    "            \"minValue\": data[i].min(),\n",
    "            \"maxValue\": data[i].max()\n",
    "        }        \n",
    "        includedFields.append(field)\n",
    "\n",
    "    search_def = {\n",
    "        \"includedFields\": includedFields,\n",
    "        \"streamDef\" : {\n",
    "            \"info\": \"Experiment\",\n",
    "            \"version\": 1,\n",
    "            \"streams\": [\n",
    "              {\n",
    "                \"info\": \"Exp\",\n",
    "                \"source\": \"file://\" + source,\n",
    "                \"columns\": [\n",
    "                  \"*\"\n",
    "                ]\n",
    "              }\n",
    "            ]\n",
    "          },\n",
    "\n",
    "        \"inferenceArgs\" : {\n",
    "            \"predictionSteps\": [\n",
    "              1\n",
    "            ],\n",
    "            \"predictedField\": predictCol\n",
    "          },\n",
    "        \"inferenceType\" : inferenceType,\n",
    "        \"iterationCount\" : iterationCount,\n",
    "        \"swarmSize\" : swarmSize\n",
    "    }\n",
    "    \n",
    "    return search_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_def = create_search_def(df, selectCols=['value'], timestampCol='timestamp', \n",
    "                               predictCol='value', \n",
    "                               source=\"ab_experiments/data/NAB/\"+PATH_relative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'includedFields': [{'fieldName': 'timestamp', 'fieldType': 'datetime'},\n",
       "  {'fieldName': 'value',\n",
       "   'fieldType': 'float',\n",
       "   'maxValue': 108.5105428,\n",
       "   'minValue': 2.0847212060000002}],\n",
       " 'inferenceArgs': {'predictedField': 'value', 'predictionSteps': [1]},\n",
       " 'inferenceType': 'TemporalAnomaly',\n",
       " 'iterationCount': -1,\n",
       " 'streamDef': {'info': 'Experiment',\n",
       "  'streams': [{'columns': ['*'],\n",
       "    'info': 'Exp',\n",
       "    'source': 'file://ab_experiments/data/NAB/realKnownCause/machine_temperature_system_failure.csv'}],\n",
       "  'version': 1},\n",
       " 'swarmSize': 'medium'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_search_def_to_file(search_def_dict):\n",
    "    \"\"\"\n",
    "    Write `search_def` dict to `search_def.json`.\n",
    "    The file will be written in the notebook folder.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    search_def_dict: dict\n",
    "        Dictionary as outputed by `create_search_def`\n",
    "        as indicated in: \n",
    "        'http://nupic.docs.numenta.org/stable/guides/swarming/running.html#a-simple-example'\n",
    "    \n",
    "    Output\n",
    "    ----------\n",
    "    `search_def.json`: `.json` file\n",
    "        File to perform swarm.\n",
    "        File located in notebook folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('search_def.json', 'w') as f:\n",
    "        json.dump(search_def_dict, f, indent=2, separators=(\", \", \": \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_search_def_to_file(search_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Open** original `.csv` in `ab_experiments/data/NAB/realKnownCause/machine_temperature_system_failure.csv` and  \n",
    "save it as `machine_temperature_system_failure_backup.csv` \n",
    "- **Edit header** of `machine_temperature_system_failure_backup.csv` adding lines #2 and #3:\n",
    "    ```\n",
    "    timestamp,value\n",
    "    datetime, float\n",
    "    T,\n",
    "    2013-12-02 21:15:00,73.96732207\n",
    "\n",
    "    ```\n",
    "- **Move** *search_def.json* and **execute** swarm algorithm:\n",
    "\n",
    "    ```\n",
    "    $ mv ab_experiments/notebooks/search_def.json ab_experiments/swarm/NAB/realKnownCaus/machine_temperature_failure\n",
    "\n",
    "    $ python scripts/run_swarm.py ab_experiments/swarm/NAB/realKnownCause/machine_temperature_failure/search_def.json --overwrite --maxWorkers=4\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "**Swarm RESULTS**  \n",
    "The best hyperparameter are stored in `ab_experiments/swarm/NAB/realKnownCause/machine_temperature_failure/swarm_medium_allData/model_0`  \n",
    "in **model_params.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load** MODEL_PARAMS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PARAMS = {'aggregationInfo': {'days': 0,\n",
    "                     'fields': [],\n",
    "                     'hours': 0,\n",
    "                     'microseconds': 0,\n",
    "                     'milliseconds': 0,\n",
    "                     'minutes': 0,\n",
    "                     'months': 0,\n",
    "                     'seconds': 0,\n",
    "                     'weeks': 0,\n",
    "                     'years': 0},\n",
    " 'model': 'HTMPrediction',\n",
    " 'modelParams': {'anomalyParams': {u'anomalyCacheRecords': None,\n",
    "                                   u'autoDetectThreshold': None,\n",
    "                                   u'autoDetectWaitRecords': None},\n",
    "                 'clParams': {'alpha': 0.03199623347319286,\n",
    "                              'regionName': 'SDRClassifierRegion',\n",
    "                              'steps': '1',\n",
    "                              'verbosity': 0},\n",
    "                 'inferenceType': 'TemporalAnomaly',\n",
    "                 'sensorParams': {'encoders': {u'timestamp_dayOfWeek': None,\n",
    "                                               u'timestamp_timeOfDay': None,\n",
    "                                               u'timestamp_weekend': {'fieldname': 'timestamp',\n",
    "                                                                      'name': 'timestamp',\n",
    "                                                                      'type': 'DateEncoder',\n",
    "                                                                      'weekend': (21,\n",
    "                                                                                  1)},\n",
    "                                               u'value': {'clipInput': True,\n",
    "                                                          'fieldname': 'value',\n",
    "                                                          'maxval': 108.5105428,\n",
    "                                                          'minval': 2.084721206,\n",
    "                                                          'n': 29,\n",
    "                                                          'name': 'value',\n",
    "                                                          'type': 'ScalarEncoder',\n",
    "                                                          'w': 21}},\n",
    "                                  'sensorAutoReset': None,\n",
    "                                  'verbosity': 0},\n",
    "                 'spEnable': True,\n",
    "                 'spParams': {'boostStrength': 0.0,\n",
    "                              'columnCount': 2048,\n",
    "                              'globalInhibition': 1,\n",
    "                              'inputWidth': 0,\n",
    "                              'numActiveColumnsPerInhArea': 40,\n",
    "                              'potentialPct': 0.8,\n",
    "                              'seed': 1956,\n",
    "                              'spVerbosity': 0,\n",
    "                              'spatialImp': 'cpp',\n",
    "                              'synPermActiveInc': 0.05,\n",
    "                              'synPermConnected': 0.1,\n",
    "                              'synPermInactiveDec': 0.06265155177806427},\n",
    "                 'tmEnable': True,\n",
    "                 'tmParams': {'activationThreshold': 12,\n",
    "                              'cellsPerColumn': 32,\n",
    "                              'columnCount': 2048,\n",
    "                              'globalDecay': 0.0,\n",
    "                              'initialPerm': 0.21,\n",
    "                              'inputWidth': 2048,\n",
    "                              'maxAge': 0,\n",
    "                              'maxSegmentsPerCell': 128,\n",
    "                              'maxSynapsesPerSegment': 32,\n",
    "                              'minThreshold': 10,\n",
    "                              'newSynapseCount': 20,\n",
    "                              'outputType': 'normal',\n",
    "                              'pamLength': 2,\n",
    "                              'permanenceDec': 0.1,\n",
    "                              'permanenceInc': 0.1,\n",
    "                              'seed': 1960,\n",
    "                              'temporalImp': 'cpp',\n",
    "                              'verbosity': 0},\n",
    "                 'trainSPNetOnlyIfRequested': False},\n",
    " 'predictAheadTime': None,\n",
    " 'version': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'labels': 0.0,\n",
       "  'timestamp': Timestamp('2013-12-02 21:15:00'),\n",
       "  'value': 73.96732207},\n",
       " {'labels': 0.0,\n",
       "  'timestamp': Timestamp('2013-12-02 21:20:00'),\n",
       "  'value': 74.93588199999998},\n",
       " {'labels': 0.0,\n",
       "  'timestamp': Timestamp('2013-12-02 21:25:00'),\n",
       "  'value': 76.12416182},\n",
       " {'labels': 0.0,\n",
       "  'timestamp': Timestamp('2013-12-02 21:30:00'),\n",
       "  'value': 78.14070732},\n",
       " {'labels': 0.0,\n",
       "  'timestamp': Timestamp('2013-12-02 21:35:00'),\n",
       "  'value': 79.32983574}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.to_dict(orient='records')\n",
    "data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "Best Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoders': {u'timestamp_dayOfWeek': None,\n",
       "  u'timestamp_timeOfDay': None,\n",
       "  u'timestamp_weekend': {'fieldname': 'timestamp',\n",
       "   'name': 'timestamp',\n",
       "   'type': 'DateEncoder',\n",
       "   'weekend': (21, 1)},\n",
       "  u'value': {'clipInput': True,\n",
       "   'fieldname': 'value',\n",
       "   'maxval': 108.5105428,\n",
       "   'minval': 2.084721206,\n",
       "   'n': 29,\n",
       "   'name': 'value',\n",
       "   'type': 'ScalarEncoder',\n",
       "   'w': 21}},\n",
       " 'sensorAutoReset': None,\n",
       " 'verbosity': 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PARAMS['modelParams']['sensorParams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp_weekend {'type': 'DateEncoder', 'fieldname': 'timestamp', 'name': 'timestamp', 'weekend': (21, 1)} \n",
      "\n",
      "0 \n",
      "\n",
      "value {'maxval': 108.5105428, 'fieldname': 'value', 'name': 'value', 'w': 21, 'clipInput': True, 'minval': 2.084721206, 'type': 'ScalarEncoder', 'n': 29} \n",
      "\n",
      "0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k,v in MODEL_PARAMS['modelParams']['sensorParams']['encoders'].items():\n",
    "    if v != None:\n",
    "        print k, v, \"\\n\"\n",
    "    else: \n",
    "        print str(0), \"\\n\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build: \n",
    "- 1x 'DateEncoder' which encodes the field `timestamp`\n",
    "- 1x 'ScalarEncoder' which encodes the field `value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dict_key(d, keys):\n",
    "    new_d = dict(d)\n",
    "    for i in keys: \n",
    "        del new_d[i]\n",
    "    return new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_weekend = remove_dict_key(MODEL_PARAMS['modelParams']['sensorParams']['encoders'].values()[0], \n",
    "                                    keys=['type','fieldname'])\n",
    "value = remove_dict_key(MODEL_PARAMS['modelParams']['sensorParams']['encoders'].values()[2], \n",
    "                        keys=['type','fieldname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nupic.encoders.random_distributed_scalar import RandomDistributedScalarEncoder\n",
    "from nupic.encoders.scalar import ScalarEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  {'maxval': 108.5105428, 'name': 'value', 'clipInput': True, 'minval': 2.084721206, 'n': 29, 'w': 21}\n",
      "73.96732207 =  [0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "74.935882 =  [0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "vEnc = ScalarEncoder(**value)\n",
    "#vEnc = ScalarEncoder(resolution=0.1, w=21, minval=60, maxval=100)\n",
    "print \"Parameters: \", value\n",
    "print str(data[0]['value']) + \" = \", vEnc.encode(data[0]['value'])\n",
    "print str(data[1]['value']) + \" = \", vEnc.encode(data[1]['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'value': ([[68.600859702249991, 68.600859702249991]], '68.60')}, ['value'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = vEnc.encode(75.123)\n",
    "print(enc)\n",
    "vEnc.decode(enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scalar encoder proposed by *swarm* is extremely rough.  \n",
    "In timestamp, *swarm* suggestede **weekend** to be a relevant feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from nupic.encoders.date import DateEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  {'name': 'timestamp', 'weekend': (21, 1)}\n",
      "TimeStamp-obs0 =  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "TimeStamp-obs1 =  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "dEnc = DateEncoder(**timestamp_weekend) \n",
    "\n",
    "print \"Parameters: \", timestamp_weekend\n",
    "\n",
    "# tsObs1 = datetime.datetime.strptime(data[0]['timestamp'], \"%Y-%m-%d %H:%M:%S\")\n",
    "print \"TimeStamp-obs0 = \", dEnc.encode(data[0]['timestamp'])\n",
    "print \"TimeStamp-obs1 = \", dEnc.encode(data[1]['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': 0.0,\n",
       " 'timestamp': Timestamp('2013-12-02 21:15:00'),\n",
       " 'value': 73.96732207}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = []\n",
    "inputVal = []\n",
    "inputTs = []\n",
    "label = []\n",
    "inputSDRval = []\n",
    "inputSDRts = []\n",
    "inputSDR = []  # main\n",
    "inputTsIdx = [0, len(dEnc.encode(data[0]['timestamp'])) - 1]\n",
    "inputValIdx = [len(dEnc.encode(data[0]['timestamp'])), \n",
    "               len(dEnc.encode(data[0]['timestamp'])) + len(vEnc.encode(data[1]['value'])) - 1]\n",
    "\n",
    "for i in xrange(len(data)):\n",
    "    obs.append(i)\n",
    "    inputTs.append(data[i]['timestamp'])  \n",
    "    inputVal.append(data[i]['value'])   \n",
    "    label.append(data[i]['labels'])\n",
    "    inputSDRts.append(dEnc.encode(data[i]['timestamp']))\n",
    "    inputSDRval.append(vEnc.encode(data[i]['value']))\n",
    "    inputSDR.append(np.hstack((inputSDRts[i], inputSDRval[i])))  # combine the 2 ancoders in 1 enoder \n",
    "    \n",
    "# send everything to dict    \n",
    "data = pd.DataFrame({'inputVal':inputVal, 'inputTs':inputTs, 'label': label,\n",
    "                      'inputSDR':inputSDR, 'inputSDRval':inputSDRval, 'inputSDRts':inputSDRts, \n",
    "                      #'inputValIdx1':inputValIdx1, 'inputVaIdx2':inputValIdx2\n",
    "                     }, index=obs).to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(inputSDR):  71\n",
      "{'inputSDR': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0], dtype=uint8), 'label': 0.0, 'inputVal': 73.96732207, 'inputTs': Timestamp('2013-12-02 21:15:00'), 'inputSDRts': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8), 'inputSDRval': array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 0, 0, 0], dtype=uint8)}\n",
      "22695\n"
     ]
    }
   ],
   "source": [
    "print \"len(inputSDR): \", len(data[0]['inputSDR'])\n",
    "print data[0]\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Pooler\n",
    "[link to wiki](http://nupic.docs.numenta.org/1.0.3/api/algorithms/spatial-pooling.html#nupic.algorithms.spatial_pooler.SpatialPooler)\n",
    "\n",
    "Load `MODEL_PARAMS` fors SP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nupic.algorithms.spatial_pooler import SpatialPooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boostStrength': 0.0,\n",
       " 'columnCount': 2048,\n",
       " 'globalInhibition': 1,\n",
       " 'inputWidth': 0,\n",
       " 'numActiveColumnsPerInhArea': 40,\n",
       " 'potentialPct': 0.8,\n",
       " 'seed': 1956,\n",
       " 'spVerbosity': 0,\n",
       " 'spatialImp': 'cpp',\n",
       " 'synPermActiveInc': 0.05,\n",
       " 'synPermConnected': 0.1,\n",
       " 'synPermInactiveDec': 0.06265155177806427}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PARAMS['modelParams']['spParams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedSPparams = {\n",
    "    'boostStrength': 0.0,\n",
    "    #'columnCount': 2048,\n",
    "    'globalInhibition': 1,\n",
    "    #'inputWidth': 0,\n",
    "    'numActiveColumnsPerInhArea': 40,\n",
    "    'potentialPct': 0.8,\n",
    "    'seed': 1956,\n",
    "    'spVerbosity': 0,\n",
    "    #'spatialImp': 'cpp',\n",
    "    'synPermActiveInc': 0.05,\n",
    "    'synPermConnected': 0.1,\n",
    "    'synPermInactiveDec': 0.06265155177806427,\n",
    "    ### Changes\n",
    "    'inputDimensions': (len(data[0]['inputSDR']), ),\n",
    "    'columnDimensions': MODEL_PARAMS['modelParams']['spParams']['columnCount'], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init SP\n",
    "sp = SpatialPooler(**selectedSPparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print SP\n",
    "\n",
    "cols = []\n",
    "connections = []\n",
    "\n",
    "for col in xrange(sp.getColumnDimensions()):\n",
    "    connected = np.zeros(len(data[0]['inputSDR']), dtype=\"int\")\n",
    "    sp.getConnectedSynapses(col, connected)\n",
    "    cols.append(col)\n",
    "    connections.append(connected)\n",
    "\n",
    "spSDR = dict(zip(cols, connections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP Shape:2048; 71\n"
     ]
    }
   ],
   "source": [
    "print \"SP Shape:\" + str(len(spSDR)) + \"; \" + str(len(spSDR[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The active bits (min-columns) are set by calculating the *overlapping score* with the input vector.  \n",
    "*Overalpping score* = `inputSDR` * `spSDR\\[column]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_active_cols(inputArr):\n",
    "    \"\"\"\n",
    "    This function takes an 1d or nd-array and returns a 1d-array with the index for ACTIVE bits/columns: \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    inputArr:   np.array (1d or nD)\n",
    "            \n",
    "    Output\n",
    "    ------\n",
    "    tmActiveColsIdx: 1d np.arraz\n",
    "        Array with index of active cols.\n",
    "        \n",
    "    \"\"\"\n",
    "    #tmObject.reshape(tmObject.numberOfCols, tm.cellsPerColumn)\n",
    "    activeColsVec = []  # initialize vector\n",
    "\n",
    "    for i in range(inputArr.shape[0]):\n",
    "        # assign 1 if any 1 (active cell) in the column,\n",
    "        # 0 otherwise\n",
    "        if np.any(inputArr[i]>0):\n",
    "        # if np.any(tm.compute(spSDR[track[3]['sp_active']], enableLearn=True, enableInference=True).reshape(256, 3)[i]>0):\n",
    "            activeColsVec.append(1)\n",
    "        else:\n",
    "            activeColsVec.append(0)\n",
    "    # return index of active Columns        \n",
    "    tmActiveColsIdx = np.flatnonzero(np.array(activeColsVec))\n",
    "    return tmActiveColsIdx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Compute` returns the to 40 active cols, as defined in:  \n",
    "`MODEL_PARAMS['modelParams']['spParams']['numActiveColumnsPerInhArea']=40`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in xrange(len(data)):\n",
    "    output = np.zeros(sp.getColumnDimensions(), dtype=\"int\")\n",
    "    sp.compute(data[i]['inputSDR'], learn=False, activeArray=output)\n",
    "    data[i]['sp_active'] = idx_active_cols(output) #save to dict\n",
    "    \n",
    "#     print \"obs\" + str(i) + \", Active col: \", str(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputSDR': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0], dtype=uint8),\n",
       " 'inputSDRts': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       " 'inputSDRval': array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 0, 0], dtype=uint8),\n",
       " 'inputTs': Timestamp('2013-12-02 21:15:00'),\n",
       " 'inputVal': 73.96732207,\n",
       " 'label': 0.0,\n",
       " 'sp_active': array([   1,    7,   17,   31,   66,   95,  145,  147,  162,  164,  185,\n",
       "         227,  231, 1703, 1746, 1775, 1794, 1797, 1818, 1822, 1867, 1937,\n",
       "        1942, 1951, 1958, 1962, 1975, 1986, 1988, 1991, 2002, 2003, 2005,\n",
       "        2012, 2019, 2026, 2032, 2039, 2044, 2047])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs22694\n",
      "Active col:  [   7   17   31   72   77   95  145  162  164  178  185  227  231 1703 1746\n",
      " 1794 1855 1867 1868 1919 1934 1937 1942 1951 1958 1962 1975 1986 1988 1991\n",
      " 2002 2003 2005 2012 2019 2026 2032 2039 2044 2047]\n",
      "obs22694\n",
      "Active col:  [   7   17   31   72   77   95  145  162  164  178  185  227  231 1703 1746\n",
      " 1794 1855 1867 1868 1919 1934 1937 1942 1951 1958 1962 1975 1986 1988 1991\n",
      " 2002 2003 2005 2012 2019 2026 2032 2039 2044 2047]\n",
      "obs22694\n",
      "Active col:  [   7   17   31   72   77   95  145  162  164  178  185  227  231 1703 1746\n",
      " 1794 1855 1867 1868 1919 1934 1937 1942 1951 1958 1962 1975 1986 1988 1991\n",
      " 2002 2003 2005 2012 2019 2026 2032 2039 2044 2047]\n",
      "obs22694\n",
      "Active col:  [   7   17   31   72   77   95  145  162  164  178  185  227  231 1703 1746\n",
      " 1794 1855 1867 1868 1919 1934 1937 1942 1951 1958 1962 1975 1986 1988 1991\n",
      " 2002 2003 2005 2012 2019 2026 2032 2039 2044 2047]\n",
      "obs22694\n",
      "Active col:  [   7   17   31   72   77   95  145  162  164  178  185  227  231 1703 1746\n",
      " 1794 1855 1867 1868 1919 1934 1937 1942 1951 1958 1962 1975 1986 1988 1991\n",
      " 2002 2003 2005 2012 2019 2026 2032 2039 2044 2047]\n"
     ]
    }
   ],
   "source": [
    "for _ in xrange(5):\n",
    "    print \"obs\" + str(i)\n",
    "    print \"Active col: \", data[i]['sp_active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permanence\n",
    "# permanence = []\n",
    "\n",
    "# for i in xrange(sp.getColumnDimensions()):\n",
    "#     p = []\n",
    "#     sp.getPermanence(i, p)\n",
    "#     permanence.append(np.array(p))\n",
    "\n",
    "# permanence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `Permanence > Threshold` we have a connection to the *inputSDR*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing SP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputSDR:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "SDR active bits [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 47 48 49 50\n",
      " 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67]\n",
      "SP active col:  [   1    7   17   31   66   95  145  147  162  164  185  227  231 1703 1746\n",
      " 1775 1794 1797 1818 1822 1867 1937 1942 1951 1958 1962 1975 1986 1988 1991\n",
      " 2002 2003 2005 2012 2019 2026 2032 2039 2044 2047]\n",
      "\n",
      "------------\n",
      "SP active col: 1; \tOverlappin bits: 16\n",
      "SP active col: 7; \tOverlappin bits: 16\n",
      "SP active col: 17; \tOverlappin bits: 17\n",
      "SP active col: 31; \tOverlappin bits: 16\n",
      "SP active col: 66; \tOverlappin bits: 16\n",
      "SP active col: 95; \tOverlappin bits: 16\n",
      "SP active col: 145; \tOverlappin bits: 18\n",
      "SP active col: 147; \tOverlappin bits: 16\n",
      "SP active col: 162; \tOverlappin bits: 17\n",
      "SP active col: 164; \tOverlappin bits: 16\n",
      "SP active col: 185; \tOverlappin bits: 17\n",
      "SP active col: 227; \tOverlappin bits: 16\n",
      "SP active col: 231; \tOverlappin bits: 17\n",
      "SP active col: 1703; \tOverlappin bits: 16\n",
      "SP active col: 1746; \tOverlappin bits: 17\n",
      "SP active col: 1775; \tOverlappin bits: 18\n",
      "SP active col: 1794; \tOverlappin bits: 18\n",
      "SP active col: 1797; \tOverlappin bits: 18\n",
      "SP active col: 1818; \tOverlappin bits: 17\n",
      "SP active col: 1822; \tOverlappin bits: 16\n",
      "SP active col: 1867; \tOverlappin bits: 20\n",
      "SP active col: 1937; \tOverlappin bits: 19\n",
      "SP active col: 1942; \tOverlappin bits: 17\n",
      "SP active col: 1951; \tOverlappin bits: 17\n",
      "SP active col: 1958; \tOverlappin bits: 17\n",
      "SP active col: 1962; \tOverlappin bits: 16\n",
      "SP active col: 1975; \tOverlappin bits: 15\n",
      "SP active col: 1986; \tOverlappin bits: 16\n",
      "SP active col: 1988; \tOverlappin bits: 16\n",
      "SP active col: 1991; \tOverlappin bits: 16\n",
      "SP active col: 2002; \tOverlappin bits: 15\n",
      "SP active col: 2003; \tOverlappin bits: 17\n",
      "SP active col: 2005; \tOverlappin bits: 16\n",
      "SP active col: 2012; \tOverlappin bits: 15\n",
      "SP active col: 2019; \tOverlappin bits: 16\n",
      "SP active col: 2026; \tOverlappin bits: 15\n",
      "SP active col: 2032; \tOverlappin bits: 16\n",
      "SP active col: 2039; \tOverlappin bits: 15\n",
      "SP active col: 2044; \tOverlappin bits: 16\n",
      "SP active col: 2047; \tOverlappin bits: 18\n"
     ]
    }
   ],
   "source": [
    "entry = 0\n",
    "\n",
    "print \"inputSDR: \", data[entry]['inputSDR']\n",
    "print \"SDR active bits\", idx_active_cols(data[entry]['inputSDR'])\n",
    "print \"SP active col: \", data[entry]['sp_active']\n",
    "print \"\\n\", \"------------\"\n",
    "\n",
    "for i in data[entry]['sp_active']:\n",
    "    print \"SP active col: \" + str(i) + \"; \\tOverlappin bits: \" + str(sum(spSDR[i] * data[entry]['inputSDR']))\n",
    "\n",
    "#print \"Pemanence winning col: \", permanence[track[0]['sp_active']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Pooler\n",
    "\n",
    "[link to wiki](http://nupic.docs.numenta.org/1.0.3/api/algorithms/sequence-memory.html#nupic.algorithms.backtracking_tm_cpp.BacktrackingTMCPP)\n",
    "\n",
    "Load `MODEL_PARAMS` fors TM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activationThreshold': 12,\n",
       " 'cellsPerColumn': 32,\n",
       " 'columnCount': 2048,\n",
       " 'globalDecay': 0.0,\n",
       " 'initialPerm': 0.21,\n",
       " 'inputWidth': 2048,\n",
       " 'maxAge': 0,\n",
       " 'maxSegmentsPerCell': 128,\n",
       " 'maxSynapsesPerSegment': 32,\n",
       " 'minThreshold': 10,\n",
       " 'newSynapseCount': 20,\n",
       " 'outputType': 'normal',\n",
       " 'pamLength': 2,\n",
       " 'permanenceDec': 0.1,\n",
       " 'permanenceInc': 0.1,\n",
       " 'seed': 1960,\n",
       " 'temporalImp': 'cpp',\n",
       " 'verbosity': 0}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PARAMS['modelParams']['tmParams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedTMparams = { \n",
    "    'activationThreshold': 12,\n",
    "    'cellsPerColumn': 32,\n",
    "    #'columnCount': 2048,\n",
    "    'globalDecay': 0.0,\n",
    "    'initialPerm': 0.21,\n",
    "    #'inputWidth': 2048,\n",
    "    'maxAge': 0,\n",
    "    'maxSegmentsPerCell': 128,\n",
    "    'maxSynapsesPerSegment': 32,\n",
    "    'minThreshold': 10,\n",
    "    'newSynapseCount': 20,\n",
    "    'outputType': 'normal',\n",
    "    'pamLength': 2,\n",
    "    'permanenceDec': 0.1,\n",
    "    'permanenceInc': 0.1,\n",
    "    'seed': 1960,\n",
    "    #'temporalImp': 'cpp',\n",
    "    'verbosity': 0,\n",
    "    ### Changes\n",
    "    'numberOfCols': MODEL_PARAMS['modelParams']['tmParams']['columnCount'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nupic.algorithms.backtracking_tm import BacktrackingTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the example we suggest to set `verbosity=5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init TM\n",
    "tm = BacktrackingTM(**selectedTMparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every input, send the ACTIVE_sp_columns to TM as 0/1 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in xrange(len(data)): #len(track)\n",
    "    # for every input, select the 'sp_active' col and get active bit in very col  \n",
    "    sp4tm = [spSDR.get(sp) for sp in data[i]['sp_active']]\n",
    "    # stack all the arrays in matrix and sum to see overlap \n",
    "    sp4tm = sum(np.array(sp4tm))\n",
    "    # if overlap 1 send active cols aove \n",
    "    sp4tm[sp4tm>0] = 1\n",
    "    \n",
    "    # send the vector with the SP active cols to TM\n",
    "    data[i]['sp4tm'] = sp4tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "tm0 = tm.compute(data[0]['sp4tm'], enableInference=True, enableLearn=True)\n",
    "print tm0.reshape(tm.numberOfCols, tm.cellsPerColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3: send the  input to the temporal memory for learning\n",
    "\n",
    "# # Send each input in the sequence in order\n",
    "# for i in xrange(len(track[:3000])):\n",
    "    \n",
    "#     # The compute method performs one step of learning and/or inference. Note:\n",
    "#     # here we just perform learning but you can perform prediction/inference and\n",
    "#     # learning in the same step if you want (online learning).\n",
    "#     tm.compute(track[i]['sp4tm'], enableLearn=True, enableInference=True)\n",
    "#     # This function prints the segments associated with every cell.$$$$\n",
    "#     # If you really want to understand the TP, uncomment this line. By following\n",
    "#     # every step you can get an excellent understanding for exactly how the TP\n",
    "#     # learns.\n",
    "#     #tm.printCells()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the first 3000 entries to learn, test on the rest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Score\n",
    "\n",
    "[link](http://nupic.docs.numenta.org/stable/guides/anomaly-detection.html)\n",
    "\n",
    "The algorithm for the anomaly score is as follows:  \n",
    "\n",
    "AS = |A_(t) - (P_(t-1) cross A_(t))|  / |A_(t)|  \n",
    "\n",
    "A_(t):   Predicted columns at time t  \n",
    "P_(t-1): Active columns at time t\n",
    "\n",
    "**Note**: Here, a “predicted column” is a column with a non-zero confidence value. This is not exactly the same as having a cell in the predicted state. For more information, refer the “predicted cells vs. confidences” section below.  \n",
    "\n",
    "...to compute the confidences for a cell, the Temporal Pooler uses the soft match count (the number of active synapses, regardless of the permanence values). Therefore, the set of columns with non-zero confidences will always be a superset of the columns containing predicted cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeRawAnomalyScore(activeColumns, prevPredictedColumns):\n",
    "  \"\"\"Computes the raw anomaly score.\n",
    "\n",
    "  The raw anomaly score is the fraction of active columns not predicted.\n",
    "\n",
    "  :param activeColumns: array of active column indices\n",
    "  :param prevPredictedColumns: array of columns indices predicted in prev step\n",
    "  :returns: anomcaly score 0..1 (float)\n",
    "  \"\"\"\n",
    "  nActiveColumns = len(activeColumns)\n",
    "  if nActiveColumns > 0:\n",
    "    # Test whether each element of a 1-D array is also present in a second\n",
    "    # array. Sum to get the total # of columns that are active and were\n",
    "    # predicted.\n",
    "    score = np.in1d(activeColumns, prevPredictedColumns).sum()\n",
    "    # Get the percent of active columns that were NOT predicted, that is\n",
    "    # our anomaly score.\n",
    "    score = (nActiveColumns - score) / float(nActiveColumns)\n",
    "  else:\n",
    "    # There are no active columns.\n",
    "    score = 0.0\n",
    "\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility routine for printing the input vector\n",
    "def formatRow(x):\n",
    "    s = ''\n",
    "    for c in range(len(x)):\n",
    "        if c > 0 and c % 10 == 0:\n",
    "            s += ' '\n",
    "        s += str(x[c])\n",
    "    s += ' '\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Likelihood\n",
    "\n",
    "``` \n",
    "from nupic.algorithms.anomaly_likelihood import AnomalyLikelihood\n",
    "\n",
    "class AnomalyLikelihood(claLearningPeriod=None,\n",
    "                       learningPeriod=288,\n",
    "                       estimationSamples=100,\n",
    "                       historicWindowSize=8640,\n",
    "                       reestimationPeriod=100)):\n",
    "\n",
    "    NOTE: Anomaly likelihood scores are reported at a flat 0.5 for\n",
    "    learningPeriod + estimationSamples iterations.\n",
    "\n",
    "    claLearningPeriod and learningPeriod are specifying the same variable,\n",
    "    although claLearningPeriod is a deprecated name for it.\n",
    "\n",
    "    :param learningPeriod: (claLearningPeriod: deprecated) - (int) the number of\n",
    "      iterations required for the algorithm to learn the basic patterns in the\n",
    "      dataset and for the anomaly score to 'settle down'. The default is based\n",
    "      on empirical observations but in reality this could be larger for more\n",
    "      complex domains. The downside if this is too large is that real anomalies\n",
    "      might get ignored and not flagged.\n",
    "\n",
    "    :param estimationSamples: (int) the number of reasonable anomaly scores\n",
    "      required for the initial estimate of the Gaussian. The default of 100\n",
    "      records is reasonable - we just need sufficient samples to get a decent\n",
    "      estimate for the Gaussian. It's unlikely you will need to tune this since\n",
    "      the Gaussian is re-estimated every 10 iterations by default.\n",
    "\n",
    "    :param historicWindowSize: (int) size of sliding window of historical\n",
    "      data points to maintain for periodic reestimation of the Gaussian. Note:\n",
    "      the default of 8640 is based on a month's worth of history at 5-minute\n",
    "      intervals.\n",
    "\n",
    "    :param reestimationPeriod: (int) how often we re-estimate the Gaussian\n",
    "      distribution. The ideal is to re-estimate every iteration but this is a\n",
    "      performance hit. In general the system is not very sensitive to this\n",
    "      number as long as it is small relative to the total number of records\n",
    "      processed.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nupic.algorithms.anomaly_likelihood import AnomalyLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalyLikelihood = AnomalyLikelihood()\n",
    "\n",
    "# for i in xrange(100):  # len(data)\n",
    "#     # Compute the Anomaly Likelihood\n",
    "#     likelihood = anomalyLikelihood.anomalyProbability(data[i]['inputVal'], data[i]['AnomalyScore'], data[i]['inputTs'])\n",
    "#     #likelihood = anomalyLikelihood.anomalyProbability(inputData[\"value\"], anomalyScore, inputData[\"dttm\"])\n",
    "#     logLikelihood = anomalyLikelihood.computeLogLikelihood(likelihood)\n",
    "#     data[i]['lh'] = likelihood    \n",
    "#     data[i]['logLH'] = likelihood\n",
    "#     #if likelihood > 0.9999:\n",
    "#         #print \"Anomaly detected:\", track[i]['inputVal'], track[i]['inputTs'], likelihood\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: send the same sequence of vectors and look at predictions made by\n",
    "# temporal memory\n",
    "\n",
    "\n",
    "for i in xrange(len(data)): \n",
    "#     print \"\\n\\n--------\" + str(i) + \"-----------\"\n",
    "#     print \"Raw input vector\\n\",formatRow(track[i]['sp4tm'])\n",
    "\n",
    "    # Send each vector to the TP, with learning turned off\n",
    "    tm.compute(data[i]['sp4tm'], enableLearn=True, enableInference=True)\n",
    "\n",
    "    # This method prints out the active state of each cell followed by the\n",
    "    # predicted state of each cell. For convenience the cells are grouped\n",
    "    # 10 at a time. When there are multiple cells per column the printout\n",
    "    # is arranged so the cells in a column are stacked together\n",
    "    #\n",
    "    # What you should notice is that the columns where active state is 1\n",
    "    # represent the SDR for the current input pattern and the columns where\n",
    "    # predicted state is 1 represent the SDR for the next expected pattern\n",
    "#     print \"\\nAll the active and predicted cells:\"\n",
    "#     tm.printStates(printPrevious=False, printLearnState=False)\n",
    "\n",
    "    # tm.getPredictedState() gets the predicted cells.\n",
    "    # predictedCells[c][i] represents the state of the i'th cell in the c'th\n",
    "    # column. To see if a column is predicted, we can simply take the OR\n",
    "    # across all the cells in that column. In numpy we can do this by taking\n",
    "    # the max along axis 1.\n",
    "#     print \"\\n\\nThe following columns are predicted by the temporal memory. This\"\n",
    "#     print \"should correspond to columns in the *next* item in the sequence.\"\n",
    "#     predictedCells = tm.getPredictedState()\n",
    "#     print formatRow(predictedCells.max(axis=1).nonzero())\n",
    "    \n",
    "    ## ANOMALY SCORE\n",
    "    data[i]['TMpredictedCells'] = tm.cellConfidence['t-1']\n",
    "    #data[i]['TMpredictedCells_2'] = tm.infPredictedState['t-1']\n",
    "    data[i]['TMactiveCells'] = tm.infActiveState['t']           \n",
    "    data[i]['AnomalyScore'] = computeRawAnomalyScore(idx_active_cols(tm.infActiveState['t']), idx_active_cols(tm.cellConfidence['t-1']))\n",
    "    #data[i]['AnomalyScore2'] = computeRawAnomalyScore(idx_active_cols(tm.infActiveState['t']), idx_active_cols(tm.infPredictedState['t-1']))\n",
    "    likelihood = anomalyLikelihood.anomalyProbability(data[i]['inputVal'], data[i]['AnomalyScore'], data[i]['inputTs'])\n",
    "    #likelihood = anomalyLikelihood.anomalyProbability(inputData[\"value\"], anomalyScore, inputData[\"dttm\"])\n",
    "    logLikelihood = anomalyLikelihood.computeLogLikelihood(likelihood)\n",
    "    data[i]['lh'] = likelihood    \n",
    "    data[i]['logLH'] = logLikelihood\n",
    "    if likelihood > 0.9999:\n",
    "        data[i]['flag'] = 1\n",
    "    else:\n",
    "        data[i]['flag'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vAnomalyScore = []\n",
    "#vAnomalyScore2 = []\n",
    "vLH = []\n",
    "vLogLH = []\n",
    "vFlag = []\n",
    "\n",
    "for i in xrange(len(data)):\n",
    "    vAnomalyScore.append(data[i].get('AnomalyScore'))    \n",
    " #   vAnomalyScore2.append(track[i].get('AnomalyScore'))    \n",
    "    vLH.append(data[i].get('lh'))\n",
    "    vLogLH.append(data[i].get('logLH'))\n",
    "    vFlag.append(data[i].get('flag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6c33e8ca10>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(vAnomalyScore).plot()\n",
    "pd.Series(vLH).plot()\n",
    "pd.Series(vLogLH).plot()\n",
    "pd.Series(vFlag).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = 0\n",
    "# b = -1\n",
    "\n",
    "# fig, ax = plt.subplots(2, sharex=True, figsize=(15,7))\n",
    "# ax[0].plot(data.value[a:b], color='b')\n",
    "# ax[0].set(ylabel='temperature_C', title='Machine Temperature Anomaly Detection')\n",
    "# ax[0].vlines(idcs, 0, 1, transform=ax[0].get_xaxis_transform(), colors='y')\n",
    "# #ax[0].vlines(anomalies_gt, 0, 1, transform=ax[0].get_xaxis_transform(), colors='b', linestyles={'dashed'})\n",
    "\n",
    "\n",
    "# ax[1].plot(data.raw_score[a:b], label='anomaly_score')\n",
    "# ax[1].set(ylabel='Anomaly Score and LH')\n",
    "# ax[1].plot(data.anomaly_score[a:b], color='r', alpha=0.5, label='anomaly_LH')\n",
    "# ax[1].axhline(y=0.5, \n",
    "#               color='r', linestyle='--', linewidth=0.4)\n",
    "#               #xmin=data.anomaly_likelihood.index[a], xmax=data.anomaly_likelihood.index[b])\n",
    "# ax[1].vlines(idcs, 0, 1, transform=ax[1].get_xaxis_transform(), colors='y', label='groundtruth')\n",
    "# ax[1].set(xlabel='time (5min)')\n",
    "# ax[1].legend(loc=1)\n",
    "\n",
    "# #set ticks every week\n",
    "# ax[1].xaxis.set_major_locator(mdates.WeekdayLocator())\n",
    "# #set major ticks format\n",
    "# ax[1].xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from nab.scorer import Scorer\n",
    "\n",
    "class Scorer(object):\n",
    "  \"\"\"Class used to score a datafile.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               timestamps,\n",
    "               predictions,\n",
    "               labels,\n",
    "               windowLimits,\n",
    "               costMatrix,\n",
    "               probationaryPeriod):\n",
    "   \n",
    "    @param predictions   (pandas.Series)   Detector predictions of\n",
    "                                           whether each record is anomalous or\n",
    "                                           not. predictions[\n",
    "                                           0:probationaryPeriod] are ignored.\n",
    "\n",
    "    @param labels        (pandas.DataFrame) Ground truth for each record.\n",
    "                                           For each record there should be a 1\n",
    "                                           or a 0. A 1 implies this record is\n",
    "                                           within an anomalous window.\n",
    "\n",
    "    @param windowLimits  (list)            All the window limits in tuple\n",
    "                                           form: (timestamp start, timestamp\n",
    "                                           end).\n",
    "\n",
    "    @param costmatrix    (dict)            Dictionary containing the\n",
    "                                           cost matrix for this profile.\n",
    "                                           type:  True positive (tp)\n",
    "                                                  False positive (fp)\n",
    "                                                  True Negative (tn)\n",
    "                                                  False Negative (fn)\n",
    "\n",
    "    @param probationaryPeriod\n",
    "                         (int)             Row index after which predictions\n",
    "                                           are scored.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Load Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# trackList = copy.deepcopy(track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to dump to .json we need to change array to lists and datetime to str\n",
    "\n",
    "for i in xrange(len(data)):\n",
    "    for k,v in data[i].items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            #print trackList[i][k].tolist\n",
    "            data[i][k] = data[i][k].tolist()\n",
    "        if isinstance(v, datetime.datetime):\n",
    "            data[i][k] = str(data[i][k])\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'univ_swarm_spLearnFalse_tmLearnAll.json'\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(data, f, indent=4, sort_keys=True, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(filename, 'r') as f:\n",
    "#         track = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-80a4aab09c73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redirecting Anomaly Score\n",
    "\n",
    "Once an AS has been outputed, we want to know which are the cells the where unpredicted and causes the error to raise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------- 4 -----------  \n",
    "Raw input vector  \n",
    "0000010000 0010010000 0   \n",
    "\n",
    "==== PY Iteration: 56 =====  \n",
    "Previous learned pattern: array([ 0,  2,  4,  5, 19])  \n",
    "  \n",
    "Active cols: [ 5 12 15]  \n",
    "Inference Active state  \n",
    "0000000000 0000000000 0  \n",
    "0000010000 0000010000 0  \n",
    "0000000000 0010000000 0  \n",
    "  \n",
    "Inference Predicted state: [1, 7, 8, 14, 20]  \n",
    "0000000000 0000000000 0  \n",
    "0100000110 0000000000 1  \n",
    "0000000000 0000100000 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backup': array([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]], dtype=int8), 'candidate': array([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]], dtype=int8), 't': array([[0, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 1, 0]], dtype=int8), 't-1': array([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]], dtype=int8)}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.infPredictedState # [t] predicted to be active next, [t-1] predicted before, for current input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  7  8 14 20] \n"
     ]
    }
   ],
   "source": [
    "predictedCells = tm.getPredictedState()\n",
    "print formatRow(predictedCells.max(axis=1).nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 1, 0]], dtype=int8)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.getPredictedState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 0 Cell 1 : 2 segment(s)\n",
      "   Seg #0   ID:10    True 0.1785714 (  10/10  )    3 [1,1]1.00 [1,2]0.50 [7,1]1.00 [8,1]1.00 [8,2]0.50 [14,1]0.50 [14,2]1.00 [20,1]1.00\n",
      "   Seg #1   ID:36    True 0.0178571 (   1/1   )   23 [1,0]0.50 [7,0]0.50 [8,0]0.50 [14,0]0.50 [20,0]0.50\n",
      "Column 0 Cell 2 : 1 segment(s)\n",
      "   Seg #0   ID:23    True 0.1964286 (  11/11  )    1 [2,1]1.00 [10,1]1.00 [12,1]1.00 [16,2]1.00 [17,2]1.00 [20,1]1.00\n",
      "Column 1 Cell 1 : 2 segment(s)\n",
      "   Seg #0   ID:0     True 0.0178571 (   1/1   )   55\n",
      "  *Seg #1   ID:31    True 0.1785714 (  10/11  )    4 [5,1]1.00 [12,2]1.00 [15,1]1.00\n",
      "Column 1 Cell 2 : 1 segment(s)\n",
      "   Seg #0   ID:5     True 0.0178571 (   1/10  )   54 [1,1]0.50 [7,2]0.50 [8,1]0.50 [14,1]0.50 [20,2]0.50\n",
      "Column 2 Cell 1 : 3 segment(s)\n",
      "   Seg #0   ID:11    True 0.1785714 (  10/10  )    3 [1,1]1.00 [1,2]0.50 [7,1]1.00 [8,1]1.00 [8,2]0.50 [14,1]0.50 [14,2]1.00 [20,1]1.00\n",
      "   Seg #1   ID:17    True 0.1964286 (  11/11  )    2 [0,1]1.00 [2,1]1.00 [6,1]1.00 [11,1]0.50 [11,2]1.00 [13,2]1.00 [14,1]0.50 [14,2]1.00 [15,1]0.50 [15,2]1.00\n",
      "   Seg #2   ID:37    True 0.0178571 (   1/1   )   23 [1,0]0.50 [7,0]0.50 [8,0]0.50 [14,0]0.50 [20,0]0.50\n",
      "Column 2 Cell 2 : 1 segment(s)\n",
      "   Seg #0   ID:24    True 0.1964286 (  11/11  )    1 [2,1]1.00 [10,1]1.00 [12,1]1.00 [16,2]1.00 [17,2]1.00 [20,1]1.00\n",
      "Column 4 Cell 2 : 1 segment(s)\n",
      "   Seg #0   ID:25    True 0.1964286 (  11/11  )    1 [2,1]1.00 [10,1]1.00 [12,1]1.00 [16,2]1.00 [17,2]1.00 [20,1]1.00\n",
      "Column 5 Cell 1 : 2 segment(s)\n",
      "   Seg #0   ID:26    True 0.1964286 (  11/11  )    1 [2,1]1.00 [10,1]1.00 [12,1]1.00 [16,2]1.00 [17,2]1.00 [20,1]1.00\n",
      "   Seg #1   ID:28    True 0.1964286 (  11/11  )    0 [0,2]1.00 [2,2]1.00 [4,2]1.00 [5,1]1.00 [19,2]1.00\n",
      "Column 6 Cell 1 : 2 segment(s)\n",
      "   Seg #0   ID:12    True 0.1785714 (  10/10  )    3 [1,1]1.00 [1,2]0.50 [7,1]1.00 [8,1]1.00 [8,2]0.50 [14,1]0.50 [14,2]1.00 [20,1]1.00\n",
      "   Seg #1   ID:38    True 0.0178571 (   1/1   )   23 [1,0]0.50 [7,0]0.50 [8,0]0.50 [14,0]0.50 [20,0]0.50\n",
      "Column 7 Cell 1 : 2 segment(s)\n",
      "   Seg #0   ID:6     True 0.0178571 (   1/10  )   54 [1,1]0.50 [7,2]0.50 [8,1]0.50 [14,1]0.50 [20,2]0.50\n",
      "  *Seg #1   ID:32    True 0.1785714 (  10/11  )    4 [5,1]1.00 [12,2]1.00 [15,1]1.00\n",
      "Column 7 Cell 2 : 1 segment(s)\n",
      "   Seg #0   ID:1     True 0.0178571 (   1/1   )   55\n",
      "Column 8 Cell 1 : 2 segment(s)\n",
      "   Seg #0   ID:2     True 0.0178571 (   1/1   )   55\n",
      "  *Seg #1   ID:33    True 0.1785714 (  10/11  )    4 [5,1]1.00 [12,2]1.00 [15,1]1.00\n",
      "Column 8 Cell 2 : 1 segment(s)\n",
      "   Seg #0   ID:7     True 0.0178571 (   1/10  )   54 [1,1]0.50 [7,2]0.50 [8,1]0.50 [14,1]0.50 [20,2]0.50\n",
      "Column 10 Cell 1 : 1 segment(s)\n",
      "   Seg #0   ID:18    True 0.1964286 (  11/11  )    2 [0,1]1.00 [2,1]1.00 [6,1]1.00 [11,1]0.50 [11,2]1.00 [13,2]1.00 [14,1]0.50 [14,2]1.00 [15,1]0.50 [15,2]1.00\n",
      "Column 11 Cell 1 : 1 segment(s)\n",
      "   Seg #0   ID:39    True 0.0178571 (   1/1   )   23 [1,0]0.50 [7,0]0.50 [8,0]0.50 [14,0]0.50 [20,0]0.50\n",
      "Column 11 Cell 2 : 1 segment(s)\n",
      "   Seg #0   ID:13    True 0.1785714 (  10/10  )    3 [1,1]1.00 [1,2]0.50 [7,1]1.00 [8,1]1.00 [8,2]0.50 [14,1]0.50 [14,2]1.00 [20,1]1.00\n",
      "Column 12 Cell 1 : 1 segment(s)\n",
      "   Seg #0   ID:19    True 0.1964286 (  11/11  )    2 [0,1]1.00 [2,1]1.00 [6,1]1.00 [11,1]0.50 [11,2]1.00 [13,2]1.00 [14,1]0.50 [14,2]1.00 [15,1]0.50 [15,2]1.00\n",
      "Column 12 Cell 2 : 1 segment(s)\n",
      "   Seg #0   ID:29    True 0.1964286 (  11/11  )    0 [0,2]1.00 [2,2]1.00 [4,2]1.00 [5,1]1.00 [19,2]1.00\n",
      "Column 13 Cell 2 : 2 segment(s)\n",
      "   Seg #0   ID:14    True 0.1785714 (  10/10  )    3 [1,1]1.00 [1,2]0.50 [7,1]1.00 [8,1]1.00 [8,2]0.50 [14,1]0.50 [14,2]1.00 [20,1]1.00\n",
      "   Seg #1   ID:40    True 0.0178571 (   1/1   )   23 [1,0]0.50 [7,0]0.50 [8,0]0.50 [14,0]0.50 [20,0]0.50\n",
      "Column 14 Cell 1 : 3 segment(s)\n",
      "   Seg #0   ID:3     True 0.0178571 (   1/1   )   55\n",
      "   Seg #1   ID:8     True 0.0178571 (   1/1   )   54 [1,1]0.50 [7,2]0.50 [8,1]0.50 [14,1]0.50 [20,2]0.50\n",
      "   Seg #2   ID:41    True 0.0178571 (   1/1   )   23 [1,0]0.50 [7,0]0.50 [8,0]0.50 [14,0]0.50 [20,0]0.50\n",
      "Column 14 Cell 2 : 2 segment(s)\n",
      "   Seg #0   ID:15    True 0.1785714 (  10/10  )    3 [1,1]1.00 [1,2]0.50 [7,1]1.00 [8,1]1.00 [8,2]0.50 [14,1]0.50 [14,2]1.00 [20,1]1.00\n",
      "  *Seg #1   ID:34    True 0.1785714 (  10/11  )    4 [5,1]1.00 [12,2]1.00 [15,1]1.00\n",
      "Column 15 Cell 1 : 2 segment(s)\n",
      "   Seg #0   ID:30    True 0.1964286 (  11/11  )    0 [0,2]1.00 [2,2]1.00 [4,2]1.00 [5,1]1.00 [19,2]1.00\n",
      "   Seg #1   ID:42    True 0.0178571 (   1/1   )   23 [1,0]0.50 [7,0]0.50 [8,0]0.50 [14,0]0.50 [20,0]0.50\n",
      "Column 15 Cell 2 : 1 segment(s)\n",
      "   Seg #0   ID:16    True 0.1785714 (  10/10  )    3 [1,1]1.00 [1,2]0.50 [7,1]1.00 [8,1]1.00 [8,2]0.50 [14,1]0.50 [14,2]1.00 [20,1]1.00\n",
      "Column 16 Cell 2 : 1 segment(s)\n",
      "   Seg #0   ID:20    True 0.1964286 (  11/11  )    2 [0,1]1.00 [2,1]1.00 [6,1]1.00 [11,1]0.50 [11,2]1.00 [13,2]1.00 [14,1]0.50 [14,2]1.00 [15,1]0.50 [15,2]1.00\n",
      "Column 17 Cell 2 : 1 segment(s)\n",
      "   Seg #0   ID:21    True 0.1964286 (  11/11  )    2 [0,1]1.00 [2,1]1.00 [6,1]1.00 [11,1]0.50 [11,2]1.00 [13,2]1.00 [14,1]0.50 [14,2]1.00 [15,1]0.50 [15,2]1.00\n",
      "Column 19 Cell 2 : 1 segment(s)\n",
      "   Seg #0   ID:27    True 0.1964286 (  11/11  )    1 [2,1]1.00 [10,1]1.00 [12,1]1.00 [16,2]1.00 [17,2]1.00 [20,1]1.00\n",
      "Column 20 Cell 1 : 3 segment(s)\n",
      "   Seg #0   ID:9     True 0.0178571 (   1/10  )   54 [1,1]0.50 [7,2]0.50 [8,1]0.50 [14,1]0.50 [20,2]0.50\n",
      "   Seg #1   ID:22    True 0.1964286 (  11/11  )    2 [0,1]1.00 [2,1]1.00 [6,1]1.00 [11,1]0.50 [11,2]1.00 [13,2]1.00 [14,1]0.50 [14,2]1.00 [15,1]0.50 [15,2]1.00\n",
      "  *Seg #2   ID:35    True 0.1785714 (  10/11  )    4 [5,1]1.00 [12,2]1.00 [15,1]1.00\n",
      "Column 20 Cell 2 : 1 segment(s)\n",
      "   Seg #0   ID:4     True 0.0178571 (   1/1   )   55\n"
     ]
    }
   ],
   "source": [
    "for c in xrange(tm.numberOfCols):\n",
    "    for i in xrange(tm.cellsPerColumn):\n",
    "        if not False or tm.infPredictedState['t'][c, i]:\n",
    "            tm.printCell(c, i, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trackability - Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'inputSDR': array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       "  'inputVal': 73.96732207,\n",
       "  'sp_active': 0},\n",
       " {'inputSDR': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       "  'inputVal': 74.93588199999998,\n",
       "  'sp_active': 2},\n",
       " {'inputSDR': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       "  'inputVal': 76.12416182,\n",
       "  'sp_active': 1},\n",
       " {'inputSDR': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], dtype=uint8),\n",
       "  'inputVal': 78.14070732,\n",
       "  'sp_active': 2},\n",
       " {'inputSDR': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0], dtype=uint8),\n",
       "  'inputVal': 79.32983574,\n",
       "  'sp_active': 3}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input SDR can be Decoded up to a certain granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.96732207"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track[0]['inputVal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'[70:80]': ([[73.888888888888886, 73.888888888888886]], '73.89')},\n",
       " ['[70:80]'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vEnc.decode(track[0]['inputSDR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed SP with `sp.compute(track[i]['inputSDR'], learn=True, activeArray=output)`,  \n",
    "and then the *Temporal Pooler* `inputSDR[track[i]['sp_active']]` = active columns in TM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trackability - Backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate *TM output* by feeding in the active columns of the *SP*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spSDR[track[0]['sp_active']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,\n",
       "        0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.compute(spSDR[track[0]['sp_active']], enableLearn=True, enableInference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return index for ACTIVE columns in TM: \n",
    "tmActive = []\n",
    "\n",
    "for i in range(tm.infActiveState['t'].shape[0]):\n",
    "    # assign 1 if any 1 (active cell) in the column,\n",
    "    # 0 otherwise\n",
    "    if np.any(tm.infActiveState['t'][i]>0):\n",
    "        tmActive.append(1)\n",
    "    else:\n",
    "        tmActive.append(0)\n",
    "# return index of active Columns        \n",
    "tm_active = np.flatnonzero(np.array(tmActive))\n",
    "del(tmActive) # delete list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  7,  8, 14, 20])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm_active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build `spSDR[track[0]['sp_active']]` back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_active = np.zeros_like(spSDR[0])\n",
    "sp_active[tm_active] = 1\n",
    "sp_active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the corresponding column in spSDR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching spSDR: [1]\n"
     ]
    }
   ],
   "source": [
    "idx = []\n",
    "for _ in spSDR:\n",
    "    i =+ 1\n",
    "    if np.array_equal(sp_active, spSDR[_]) == True:\n",
    "        idx.append(i)\n",
    "print \"matching spSDR:\", idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap:  [0, 1, 2, 1, 0]\n",
      "InputSDR[idx]:  2\n",
      "inputSDR:  [0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# calculate with which inputSDR, the active SP col has the higher overlap: \n",
    "\n",
    "\n",
    "for j in idx:\n",
    "    overlap = []\n",
    "    for i in xrange(len(track)):\n",
    "        overlap.append(sum(track[i]['inputSDR'] * spSDR[j]))\n",
    "        o = last_max_index(overlap)\n",
    "        \n",
    "    print \"overlap: \", str(overlap) +  \"\\nInputSDR[idx]: \", str(o)\n",
    "    print \"inputSDR: \", str(track[o]['inputSDR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function should be useful in case we have ties in the overlap-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_back_SP_to_SDR(lista):\n",
    "    '''\n",
    "    This fnc returns the indeces of the InputSDR/s that match \n",
    "    (have the highest overalpping score) the current SP the most.\n",
    "    \n",
    "    input:  copy of a list `list[:]` with the overlap score bw. \n",
    "            the winning spSDR[i] and the inputSDR[0:]   \n",
    "    output: 'match', a list, indeces of InputSDR in `track`\n",
    "    '''\n",
    "    \n",
    "    a = max(lista)\n",
    "    b = a\n",
    "    match = []\n",
    "    count = 0\n",
    "\n",
    "    while b == a:\n",
    "        i = lista.index(b)\n",
    "        out = lista.pop(i)\n",
    "        i = i+count  # fill the indexes popped out\n",
    "        match.append(i)\n",
    "        count += 1\n",
    "        b = max(lista)    \n",
    "    \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_back_SP_to_SDR(overlap[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputVal[2]: 76.12416182\n",
      "De-Encoder: ({'[70:80]': ([[76.111111111111114, 76.111111111111114]], '76.11')}, ['[70:80]'])\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for i in match_back_SP_to_SDR(overlap[:]):\n",
    "    print \"inputVal[\" + str(i) + \"]: \" + str(track[i]['inputVal'])\n",
    "    print \"De-Encoder: \" + str(vEnc.decode(track[i]['inputSDR']))\n",
    "    print \"-------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mleborgne/_git/nupic/src/nupic/datafiles/extra/hotgym/hotgym.csv\n",
      "\n",
      "gym,address,timestamp,consumption\n",
      "string,string,datetime,float\n",
      "S,,T,\n",
      "Balgowlah Platinum,Shop 67 197-215 Condamine Street Balgowlah 2093,2010-07-02 00:00:00.0,5.3\n",
      "Balgowlah Platinum,Shop 67 197-215 Condamine Street Balgowlah 2093,2010-07-02 00:15:00.0,5.5\n",
      "Balgowlah Platinum,Shop 67 197-215 Condamine Street Balgowlah 2093,2010-07-02 00:30:00.0,5.1\n",
      "Balgowlah Platinum,Shop 67 197-215 Condamine Street Balgowlah 2093,2010-07-02 00:45:00.0,5.3\n",
      "Balgowlah Platinum,Shop 67 197-215 Condamine Street Balgowlah 2093,2010-07-02 01:00:00.0,5.2\n"
     ]
    }
   ],
   "source": [
    "from pkg_resources import resource_filename\n",
    "\n",
    "datasetPath = resource_filename(\"nupic.datafiles\", \"extra/hotgym/hotgym.csv\")\n",
    "print datasetPath\n",
    "\n",
    "with open(datasetPath) as inputFile:\n",
    "    print\n",
    "    for _ in xrange(8):\n",
    "        print inputFile.next().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "`FileRecordStream` - file reader for the NuPIC file format (CSV with three header rows, understands datetimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Balgowlah Platinum', 'Shop 67 197-215 Condamine Street Balgowlah 2093', datetime.datetime(2010, 7, 2, 0, 0), 5.3]\n",
      "['Balgowlah Platinum', 'Shop 67 197-215 Condamine Street Balgowlah 2093', datetime.datetime(2010, 7, 2, 0, 15), 5.5]\n",
      "['Balgowlah Platinum', 'Shop 67 197-215 Condamine Street Balgowlah 2093', datetime.datetime(2010, 7, 2, 0, 30), 5.1]\n",
      "['Balgowlah Platinum', 'Shop 67 197-215 Condamine Street Balgowlah 2093', datetime.datetime(2010, 7, 2, 0, 45), 5.3]\n",
      "['Balgowlah Platinum', 'Shop 67 197-215 Condamine Street Balgowlah 2093', datetime.datetime(2010, 7, 2, 1, 0), 5.2]\n"
     ]
    }
   ],
   "source": [
    "from nupic.data.file_record_stream import FileRecordStream\n",
    "\n",
    "def getData():\n",
    "    return FileRecordStream(datasetPath)\n",
    "\n",
    "data = getData()\n",
    "for _ in xrange(5):\n",
    "    print data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nupic.frameworks.opf.model_factory import ModelFactory\n",
    "model = ModelFactory.create(MODEL_PARAMS)\n",
    "model.enableInference({'predictedField': 'consumption'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  5.3\n",
      "prediction:  5.3\n",
      "input:  5.5\n",
      "prediction:  5.5\n",
      "input:  5.1\n",
      "prediction:  5.36\n",
      "input:  5.3\n",
      "prediction:  5.1\n",
      "input:  5.2\n",
      "prediction:  5.342\n",
      "input:  5.5\n",
      "prediction:  5.2994\n",
      "input:  4.5\n",
      "prediction:  5.35958\n",
      "input:  1.2\n",
      "prediction:  4.92\n",
      "input:  1.1\n",
      "prediction:  1.2\n",
      "input:  1.2\n",
      "prediction:  1.17\n",
      "input:  1.2\n",
      "prediction:  1.179\n",
      "input:  1.2\n",
      "prediction:  1.1853\n",
      "input:  1.2\n",
      "prediction:  1.18971\n",
      "input:  1.2\n",
      "prediction:  1.192797\n",
      "input:  1.1\n",
      "prediction:  1.1949579\n",
      "input:  1.2\n",
      "prediction:  1.16647053\n",
      "input:  1.1\n",
      "prediction:  1.176529371\n",
      "input:  1.2\n",
      "prediction:  1.1535705597\n",
      "input:  1.2\n",
      "prediction:  1.16749939179\n",
      "input:  1.1\n",
      "prediction:  1.17724957425\n",
      "input:  1.2\n",
      "prediction:  1.15407470198\n",
      "input:  6.0\n",
      "prediction:  1.16785229138\n",
      "input:  7.9\n",
      "prediction:  5.551706\n",
      "input:  8.4\n",
      "prediction:  6.2561942\n",
      "input:  10.6\n",
      "prediction:  6.89933594\n",
      "input:  12.4\n",
      "prediction:  10.6\n",
      "input:  12.1\n",
      "prediction:  12.4\n",
      "input:  12.4\n",
      "prediction:  12.31\n",
      "input:  11.4\n",
      "prediction:  12.337\n",
      "input:  11.2\n",
      "prediction:  10.84\n",
      "input:  10.8\n",
      "prediction:  10.948\n",
      "input:  12.0\n",
      "prediction:  10.9036\n",
      "input:  11.8\n",
      "prediction:  11.23252\n",
      "input:  11.9\n",
      "prediction:  11.402764\n",
      "input:  11.4\n",
      "prediction:  11.5519348\n",
      "input:  11.0\n",
      "prediction:  11.50635436\n",
      "input:  9.8\n",
      "prediction:  11.354448052\n",
      "input:  9.8\n",
      "prediction:  10.8881136364\n",
      "input:  10.8\n",
      "prediction:  10.5616795455\n",
      "input:  11.1\n",
      "prediction:  10.6331756818\n",
      "input:  11.1\n",
      "prediction:  10.7732229773\n",
      "input:  11.0\n",
      "prediction:  10.8712560841\n",
      "input:  10.7\n",
      "prediction:  10.9098792589\n",
      "input:  10.6\n",
      "prediction:  10.8469154812\n",
      "input:  10.3\n",
      "prediction:  10.7728408368\n",
      "input:  10.1\n",
      "prediction:  10.6309885858\n",
      "input:  12.9\n",
      "prediction:  10.4716920101\n",
      "input:  10.5\n",
      "prediction:  10.4716920101\n",
      "input:  9.7\n",
      "prediction:  10.480184407\n",
      "input:  9.7\n",
      "prediction:  10.2461290849\n",
      "input:  9.2\n",
      "prediction:  10.0822903594\n",
      "input:  9.2\n",
      "prediction:  9.81760325161\n",
      "input:  9.2\n",
      "prediction:  9.63232227613\n",
      "input:  9.3\n",
      "prediction:  9.50262559329\n",
      "input:  9.1\n",
      "prediction:  9.4418379153\n",
      "input:  9.0\n",
      "prediction:  9.33928654071\n",
      "input:  8.9\n",
      "prediction:  9.2375005785\n",
      "input:  9.0\n",
      "prediction:  9.13625040495\n",
      "input:  8.9\n",
      "prediction:  9.09537528346\n",
      "input:  8.9\n",
      "prediction:  9.03676269843\n",
      "input:  9.0\n",
      "prediction:  8.9957338889\n",
      "input:  9.2\n",
      "prediction:  8.99701372223\n",
      "input:  10.0\n",
      "prediction:  9.05790960556\n",
      "input:  10.7\n",
      "prediction:  9.34053672389\n",
      "input:  8.9\n",
      "prediction:  9.74837570672\n",
      "input:  9.0\n",
      "prediction:  9.49386299471\n",
      "input:  9.0\n",
      "prediction:  9.34570409629\n",
      "input:  9.3\n",
      "prediction:  9.24199286741\n",
      "input:  9.3\n",
      "prediction:  9.25939500718\n",
      "input:  9.1\n",
      "prediction:  9.27157650503\n",
      "input:  9.1\n",
      "prediction:  9.22010355352\n",
      "input:  9.1\n",
      "prediction:  9.18407248746\n",
      "input:  9.2\n",
      "prediction:  9.15885074122\n",
      "input:  9.4\n",
      "prediction:  9.17119551886\n",
      "input:  9.3\n",
      "prediction:  9.2398368632\n",
      "input:  9.3\n",
      "prediction:  9.25788580424\n",
      "input:  9.1\n",
      "prediction:  9.27052006297\n",
      "input:  9.1\n",
      "prediction:  9.21936404408\n",
      "input:  11.0\n",
      "prediction:  9.18355483085\n",
      "input:  9.0\n",
      "prediction:  9.7284883816\n",
      "input:  8.6\n",
      "prediction:  9.50994186712\n",
      "input:  3.0\n",
      "prediction:  9.50994186712\n",
      "input:  1.3\n",
      "prediction:  4.344\n",
      "input:  1.2\n",
      "prediction:  1.20749660397\n",
      "input:  1.3\n",
      "prediction:  1.20524762278\n",
      "input:  1.3\n",
      "prediction:  1.23367333594\n",
      "input:  1.3\n",
      "prediction:  1.25357133516\n",
      "input:  1.2\n",
      "prediction:  1.26749993461\n",
      "input:  1.3\n",
      "prediction:  1.24724995423\n",
      "input:  1.2\n",
      "prediction:  1.26307496796\n",
      "input:  1.3\n",
      "prediction:  1.24415247757\n",
      "input:  1.2\n",
      "prediction:  1.2609067343\n",
      "input:  1.3\n",
      "prediction:  1.24263471401\n",
      "input:  1.2\n",
      "prediction:  1.25984429981\n",
      "input:  1.1\n",
      "prediction:  1.24189100987\n",
      "input:  2.3\n",
      "prediction:  1.19932370691\n",
      "input:  5.5\n",
      "prediction:  3.7308\n",
      "input:  5.5\n",
      "prediction:  6.8366746106\n",
      "input:  5.8\n",
      "prediction:  6.43567222742\n",
      "input:  5.7\n",
      "prediction:  6.24497055919\n"
     ]
    }
   ],
   "source": [
    "data = getData()\n",
    "for _ in xrange(100):\n",
    "    record = dict(zip(data.getFieldNames(), data.next()))\n",
    "    print \"input: \", record[\"consumption\"]\n",
    "    result = model.run(record)\n",
    "    print \"prediction: \", result.inferences[\"multiStepBestPredictions\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-step prediction:  1.19932370691\n"
     ]
    }
   ],
   "source": [
    "print \"5-step prediction: \", result.inferences[\"multiStepBestPredictions\"][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Params!\n",
    "MODEL_PARAMS = {\n",
    "    # Type of model that the rest of these parameters apply to.\n",
    "    'model': \"HTMPrediction\",\n",
    "\n",
    "    # Version that specifies the format of the config.\n",
    "    'version': 1,\n",
    "\n",
    "    # Intermediate variables used to compute fields in modelParams and also\n",
    "    # referenced from the control section.\n",
    "    'aggregationInfo': {   'days': 0,\n",
    "        'fields': [('consumption', 'sum')],\n",
    "        'hours': 1,\n",
    "        'microseconds': 0,\n",
    "        'milliseconds': 0,\n",
    "        'minutes': 0,\n",
    "        'months': 0,\n",
    "        'seconds': 0,\n",
    "        'weeks': 0,\n",
    "        'years': 0},\n",
    "\n",
    "    'predictAheadTime': None,\n",
    "\n",
    "    # Model parameter dictionary.\n",
    "    'modelParams': {\n",
    "        # The type of inference that this model will perform\n",
    "        'inferenceType': 'TemporalAnomaly',\n",
    "\n",
    "        'sensorParams': {\n",
    "            # Sensor diagnostic output verbosity control;\n",
    "            # if > 0: sensor region will print out on screen what it's sensing\n",
    "            # at each step 0: silent; >=1: some info; >=2: more info;\n",
    "            # >=3: even more info (see compute() in py/regions/RecordSensor.py)\n",
    "            'verbosity' : 0,\n",
    "\n",
    "            # Include the encoders we use\n",
    "            'encoders': {\n",
    "                u'timestamp_timeOfDay': {\n",
    "                    'fieldname': u'timestamp',\n",
    "                    'name': u'timestamp_timeOfDay',\n",
    "                    'timeOfDay': (21, 0.5),\n",
    "                    'type': 'DateEncoder'},\n",
    "                u'timestamp_dayOfWeek': None,\n",
    "                u'timestamp_weekend': None,\n",
    "                u'consumption': {\n",
    "                    'clipInput': True,\n",
    "                    'fieldname': u'consumption',\n",
    "                    'maxval': 100.0,\n",
    "                    'minval': 0.0,\n",
    "                    'n': 50,\n",
    "                    'name': u'c1',\n",
    "                    'type': 'ScalarEncoder',\n",
    "                    'w': 21},},\n",
    "\n",
    "            # A dictionary specifying the period for automatically-generated\n",
    "            # resets from a RecordSensor;\n",
    "            #\n",
    "            # None = disable automatically-generated resets (also disabled if\n",
    "            # all of the specified values evaluate to 0).\n",
    "            # Valid keys is the desired combination of the following:\n",
    "            #   days, hours, minutes, seconds, milliseconds, microseconds, weeks\n",
    "            #\n",
    "            # Example for 1.5 days: sensorAutoReset = dict(days=1,hours=12),\n",
    "            #\n",
    "            # (value generated from SENSOR_AUTO_RESET)\n",
    "            'sensorAutoReset' : None,\n",
    "        },\n",
    "\n",
    "        'spEnable': True,\n",
    "\n",
    "        'spParams': {\n",
    "            # SP diagnostic output verbosity control;\n",
    "            # 0: silent; >=1: some info; >=2: more info;\n",
    "            'spVerbosity' : 0,\n",
    "\n",
    "            # Spatial Pooler implementation selector, see getSPClass\n",
    "            # in py/regions/SPRegion.py for details\n",
    "            # 'py' (default), 'cpp' (speed optimized, new)\n",
    "            'spatialImp' : 'cpp',\n",
    "\n",
    "            'globalInhibition': 1,\n",
    "\n",
    "            # Number of cell columns in the cortical region (same number for\n",
    "            # SP and TM)\n",
    "            # (see also tpNCellsPerCol)\n",
    "            'columnCount': 2048,\n",
    "\n",
    "            'inputWidth': 0,\n",
    "\n",
    "            # SP inhibition control (absolute value);\n",
    "            # Maximum number of active columns in the SP region's output (when\n",
    "            # there are more, the weaker ones are suppressed)\n",
    "            'numActiveColumnsPerInhArea': 40,\n",
    "\n",
    "            'seed': 1956,\n",
    "\n",
    "            # potentialPct\n",
    "            # What percent of the columns's receptive field is available\n",
    "            # for potential synapses. At initialization time, we will\n",
    "            # choose potentialPct * (2*potentialRadius+1)^2\n",
    "            'potentialPct': 0.5,\n",
    "\n",
    "            # The default connected threshold. Any synapse whose\n",
    "            # permanence value is above the connected threshold is\n",
    "            # a \"connected synapse\", meaning it can contribute to the\n",
    "            # cell's firing. Typical value is 0.10. Cells whose activity\n",
    "            # level before inhibition falls below minDutyCycleBeforeInh\n",
    "            # will have their own internal synPermConnectedCell\n",
    "            # threshold set below this default value.\n",
    "            # (This concept applies to both SP and TM and so 'cells'\n",
    "            # is correct here as opposed to 'columns')\n",
    "            'synPermConnected': 0.1,\n",
    "\n",
    "            'synPermActiveInc': 0.1,\n",
    "\n",
    "            'synPermInactiveDec': 0.005,\n",
    "        },\n",
    "\n",
    "        # Controls whether TM is enabled or disabled;\n",
    "        # TM is necessary for making temporal predictions, such as predicting\n",
    "        # the next inputs.  Without TP, the model is only capable of\n",
    "        # reconstructing missing sensor inputs (via SP).\n",
    "        'tmEnable' : True,\n",
    "\n",
    "        'tmParams': {\n",
    "            # TM diagnostic output verbosity control;\n",
    "            # 0: silent; [1..6]: increasing levels of verbosity\n",
    "            # (see verbosity in nupic/trunk/py/nupic/research/TP.py and BacktrackingTMCPP.py)\n",
    "            'verbosity': 0,\n",
    "\n",
    "            # Number of cell columns in the cortical region (same number for\n",
    "            # SP and TM)\n",
    "            # (see also tpNCellsPerCol)\n",
    "            'columnCount': 2048,\n",
    "\n",
    "            # The number of cells (i.e., states), allocated per column.\n",
    "            'cellsPerColumn': 32,\n",
    "\n",
    "            'inputWidth': 2048,\n",
    "\n",
    "            'seed': 1960,\n",
    "\n",
    "            # Temporal Pooler implementation selector (see _getTPClass in\n",
    "            # CLARegion.py).\n",
    "            'temporalImp': 'cpp',\n",
    "\n",
    "            # New Synapse formation count\n",
    "            # NOTE: If None, use spNumActivePerInhArea\n",
    "            #\n",
    "            # TODO: need better explanation\n",
    "            'newSynapseCount': 20,\n",
    "\n",
    "            # Maximum number of synapses per segment\n",
    "            #  > 0 for fixed-size CLA\n",
    "            # -1 for non-fixed-size CLA\n",
    "            #\n",
    "            # TODO: for Ron: once the appropriate value is placed in TP\n",
    "            # constructor, see if we should eliminate this parameter from\n",
    "            # description.py.\n",
    "            'maxSynapsesPerSegment': 32,\n",
    "\n",
    "            # Maximum number of segments per cell\n",
    "            #  > 0 for fixed-size CLA\n",
    "            # -1 for non-fixed-size CLA\n",
    "            #\n",
    "            # TODO: for Ron: once the appropriate value is placed in TP\n",
    "            # constructor, see if we should eliminate this parameter from\n",
    "            # description.py.\n",
    "            'maxSegmentsPerCell': 128,\n",
    "\n",
    "            # Initial Permanence\n",
    "            # TODO: need better explanation\n",
    "            'initialPerm': 0.21,\n",
    "\n",
    "            # Permanence Increment\n",
    "            'permanenceInc': 0.1,\n",
    "\n",
    "            # Permanence Decrement\n",
    "            # If set to None, will automatically default to tpPermanenceInc\n",
    "            # value.\n",
    "            'permanenceDec' : 0.1,\n",
    "\n",
    "            'globalDecay': 0.0,\n",
    "\n",
    "            'maxAge': 0,\n",
    "\n",
    "            # Minimum number of active synapses for a segment to be considered\n",
    "            # during search for the best-matching segments.\n",
    "            # None=use default\n",
    "            # Replaces: tpMinThreshold\n",
    "            'minThreshold': 9,\n",
    "\n",
    "            # Segment activation threshold.\n",
    "            # A segment is active if it has >= tpSegmentActivationThreshold\n",
    "            # connected synapses that are active due to infActiveState\n",
    "            # None=use default\n",
    "            # Replaces: tpActivationThreshold\n",
    "            'activationThreshold': 12,\n",
    "\n",
    "            'outputType': 'normal',\n",
    "\n",
    "            # \"Pay Attention Mode\" length. This tells the TM how many new\n",
    "            # elements to append to the end of a learned sequence at a time.\n",
    "            # Smaller values are better for datasets with short sequences,\n",
    "            # higher values are better for datasets with long sequences.\n",
    "            'pamLength': 1,\n",
    "        },\n",
    "\n",
    "        'clParams': {\n",
    "            'regionName' : 'SDRClassifierRegion',\n",
    "\n",
    "            # Classifier diagnostic output verbosity control;\n",
    "            # 0: silent; [1..6]: increasing levels of verbosity\n",
    "            'verbosity' : 0,\n",
    "\n",
    "            # This controls how fast the classifier learns/forgets. Higher values\n",
    "            # make it adapt faster and forget older patterns faster.\n",
    "            'alpha': 0.005,\n",
    "\n",
    "            # This is set after the call to updateConfigFromSubConfig and is\n",
    "            # computed from the aggregationInfo and predictAheadTime.\n",
    "            'steps': '1',\n",
    "\n",
    "            'implementation': 'cpp',\n",
    "        },\n",
    "\n",
    "        'anomalyParams': {\n",
    "            u'anomalyCacheRecords': None,\n",
    "            u'autoDetectThreshold': None,\n",
    "            u'autoDetectWaitRecords': 2184\n",
    "        },\n",
    "\n",
    "        'trainSPNetOnlyIfRequested': False,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nupic.frameworks.opf.model_factory import ModelFactory\n",
    "model = ModelFactory.create(MODEL_PARAMS)\n",
    "model.enableInference({'predictedField': 'consumption'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  5.3\n",
      "prediction:  5.3\n",
      "input:  5.5\n",
      "prediction:  5.5\n",
      "input:  5.1\n",
      "prediction:  5.36\n",
      "input:  5.3\n",
      "prediction:  5.1\n",
      "input:  5.2\n",
      "prediction:  5.342\n"
     ]
    }
   ],
   "source": [
    "data = getData()\n",
    "for _ in xrange(5):\n",
    "    record = dict(zip(data.getFieldNames(), data.next()))\n",
    "    print \"input: \", record[\"consumption\"]\n",
    "    result = model.run(record)\n",
    "    print \"prediction: \", result.inferences[\"multiStepBestPredictions\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResult(\tpredictionNumber=4\n",
      "\trawInput={'timestamp': datetime.datetime(2010, 7, 2, 1, 0), 'gym': 'Balgowlah Platinum', 'consumption': 5.2, 'address': 'Shop 67 197-215 Condamine Street Balgowlah 2093'}\n",
      "\tsensorInput=SensorInput(\tdataRow=(5.2, 1.0)\n",
      "\tdataDict={'timestamp': datetime.datetime(2010, 7, 2, 1, 0), 'gym': 'Balgowlah Platinum', 'consumption': 5.2, 'address': 'Shop 67 197-215 Condamine Street Balgowlah 2093'}\n",
      "\tdataEncodings=[array([ 0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32), array([ 0.,  0.,  0., ...,  0.,  0.,  0.], dtype=float32)]\n",
      "\tsequenceReset=0.0\n",
      "\tcategory=-1\n",
      ")\n",
      "\tinferences={'multiStepPredictions': {1: {5.1: 0.0088801263517415546, 5.2: 0.010775254623541418, 5.341999999999999: 0.98034461902471692}}, 'multiStepBucketLikelihoods': {1: {1: 0.0088801263517415546, 2: 0.98034461902471692}}, 'multiStepBestPredictions': {1: 5.341999999999999}, 'anomalyLabel': '[]', 'anomalyScore': 0.40000001}\n",
      "\tmetrics=None\n",
      "\tpredictedFieldIdx=0\n",
      "\tpredictedFieldName=consumption\n",
      "\tclassifierInput=ClassifierInput(\tdataRow=5.2\n",
      "\tbucketIndex=2\n",
      ")\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly score:  0.4\n"
     ]
    }
   ],
   "source": [
    "print \"anomaly score: \", result.inferences[\"anomalyScore\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__See Subutai's talk for more info on anomaly detection!__\n",
    "\n",
    "# Built-in OPF Clients\n",
    "\n",
    "`python examples/opf/bin/OpfRunExperiment.py examples/opf/experiments/multistep/hotgym/`\n",
    "\n",
    "Outputs `examples/opf/experiments/multistep/hotgym/inference/DefaultTask.TemporalMultiStep.predictionLog.csv`\n",
    "\n",
    "`python bin/run_swarm.py examples/opf/experiments/multistep/hotgym/permutations.py`\n",
    "\n",
    "Outputs `examples/opf/experiments/multistep/hotgym/model_0/description.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nupic",
   "language": "python",
   "name": "nupic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
